{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "a0cf2d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 shuffle\n",
    "# import random\n",
    "# with open('./train_hate_dataset_v1.txt', 'r', encoding='utf-8') as f:\n",
    "#     file = f.read()\n",
    "# files = file.split('\\n')\n",
    "# with open('./test_hate_dataset_v1.txt', 'r', encoding='utf-8') as f:\n",
    "#     file2 = f.read()\n",
    "# files2 = file2.split('\\n')\n",
    "# files.extend(files2)\n",
    "# random.shuffle(files)\n",
    "# sentences = [x.split('\\t') for x in files]\n",
    "# len_sentences = len(sentences)\n",
    "# splited_len = round(len_sentences*0.9)\n",
    "# train_sentences = sentences[:splited_len]\n",
    "# test_sentences = sentences[splited_len:]\n",
    "# train_sentences = pd.DataFrame(train_sentences)\n",
    "# test_sentences = pd.DataFrame(test_sentences)\n",
    "# train_sentences.to_csv('./train_hate_dataset_v2.txt', sep='\\t', index=None, header=None)\n",
    "# test_sentences.to_csv('./test_hate_dataset_v2.txt', sep='\\t', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7140b5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import jamotools #자모 단위 토큰화\n",
    "import re\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "from kobert.utils import get_tokenizer\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, mode):\n",
    "        self.sentences = [line[sent_idx] for line in dataset]\n",
    "        self.labels = [int(line[label_idx]) for line in dataset]\n",
    "        self.korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "        self.mode = mode\n",
    "        tok_path = '/home/yeonsik/kobert/kobert_news_wiki_ko_cased-1087f8699e.spiece'\n",
    "        self.sp = SentencepieceTokenizer(tok_path)\n",
    "        self.vocab = self.make_vocab()\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.q3 = self.get_q3()\n",
    "        self.char2idx = {u:i for i, u in enumerate(self.vocab)}\n",
    "        self.idx2char = {i:u for i, u in enumerate(self.vocab)}\n",
    "        self.max_len = self.find_max_len()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.preprocess_sentence(self.sentences[i]), torch.tensor(self.labels[i]).to(torch.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def make_vocab(self):\n",
    "        vocab = ''\n",
    "        for sentence in self.sentences:\n",
    "            vocab+=' '+sentence\n",
    "        vocab = self.make_token(vocab)\n",
    "        vocab = set(vocab)\n",
    "        vocab = sorted(vocab)\n",
    "        vocab.append('<UNK>') #######\n",
    "        vocab.append('<PAD>')\n",
    "        return vocab\n",
    "    \n",
    "    def make_token(self, sentence):\n",
    "        if self.mode == 'jamo':\n",
    "            chars = self.korean.sub('', jamotools.split_syllables(sentence))\n",
    "            return list(chars)\n",
    "        elif self.mode == 'char':\n",
    "            chars = self.korean.sub('', sentence)\n",
    "            return list(chars)\n",
    "        elif self.mode == 'sentencepiece':\n",
    "            return self.sp(sentence)\n",
    "        \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        chars = self.make_token(sentence)\n",
    "        if len(chars) < self.q3:\n",
    "            need_pad = self.q3 - len(chars)\n",
    "            chars.extend(['<PAD>']*need_pad)\n",
    "        else:\n",
    "            chars = chars[:self.q3]\n",
    "        chars = torch.tensor([self.char2idx[x] for x in chars]).to(torch.int64)\n",
    "        return chars\n",
    "    \n",
    "    def find_max_len(self):\n",
    "        return max(len(self.make_token(item)) for item in self.sentences)\n",
    "    \n",
    "    def find_max_idx(self):\n",
    "        return self.sentences[np.argmax([len(self.make_token(item)) for item in self.sentences])]\n",
    "\n",
    "    \n",
    "    def get_q3(self):\n",
    "        values = np.array([len(self.make_token(x)) for x in self.sentences])\n",
    "        return int(np.quantile(values, 1.0))\n",
    "    \n",
    "    \n",
    "    def plot_len(self):\n",
    "        values = np.array([len(self.make_token(x)) for x in self.sentences])\n",
    "        plt.hist(values, density=True, bins=80)\n",
    "        plt.ylabel('count')\n",
    "        plt.xlabel('length of sequence')\n",
    "        plt.show()\n",
    "        print('문장 최대 길이 :',self.max_len)\n",
    "        results = stats.describe(values)\n",
    "        print('min={}, max={}, mean={}, Q2={} Q3={}'.format(results[1][0], results[1][1], results[2],\n",
    "                                                          np.median(values), np.quantile(values, 0.75)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd27259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, filters, embedding_dim, num_of_kernel):\n",
    "        super().__init__()\n",
    "        self.filters=filters\n",
    "        self.dropout_prob = 0.5\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_of_kernel = num_of_kernel\n",
    "        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(self.num_of_kernel)\n",
    "        for i in range(len(self.filters)):\n",
    "            conv = nn.Conv1d(in_channels=self.embedding_dim, out_channels=self.num_of_kernel, kernel_size=self.filters[i])\n",
    "            setattr(self, f'conv_{i}', conv)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=self.dropout_prob)\n",
    "        self.bn2 = nn.BatchNorm1d(self.num_of_kernel*len(self.filters))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_of_kernel*len(self.filters), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def get_conv(self, i):\n",
    "        return getattr(self, f'conv_{i}')\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        x = self.embedding(inp)\n",
    "        x = x.permute(0, 2, 1) ### embedding 을 transpose해줘야함.안하면 1d-conv이 seq 방향이 아닌, 임베딩 방향으로 진행됨.\n",
    "        conv_results = [\n",
    "            F.relu(self.bn1(self.get_conv(i)(x))).permute(0,2,1).max(1)[0]\n",
    "        for i in range(len(self.filters))]\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b329fd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "mode : char\n",
      "lr : 4e-05\n",
      "embedding_dim : 1000\n",
      "filters : [4, 5, 6]\n",
      "num_of_kernel : 100\n",
      "================\n",
      "epoch 1 train accuracy 0.702 f1_score 0.483 precision 0.626 recall 0.393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeonsik/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 test accuracy 0.633 f1_score 0.000 precision 0.000 recall 0.000\n",
      "epoch 2 train accuracy 0.771 f1_score 0.615 precision 0.759 recall 0.518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yeonsik/miniconda3/envs/nlp/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 test accuracy 0.633 f1_score 0.000 precision 0.000 recall 0.000\n",
      "epoch 3 train accuracy 0.801 f1_score 0.675 precision 0.799 recall 0.584\n",
      "epoch 3 test accuracy 0.598 f1_score 0.381 precision 0.438 recall 0.338\n",
      "epoch 4 train accuracy 0.816 f1_score 0.707 precision 0.808 recall 0.628\n",
      "epoch 4 test accuracy 0.642 f1_score 0.175 precision 0.565 recall 0.104\n",
      "epoch 5 train accuracy 0.840 f1_score 0.751 precision 0.840 recall 0.679\n",
      "epoch 5 test accuracy 0.602 f1_score 0.414 precision 0.450 recall 0.383\n",
      "epoch 6 train accuracy 0.849 f1_score 0.763 precision 0.857 recall 0.688\n",
      "epoch 6 test accuracy 0.426 f1_score 0.503 precision 0.369 recall 0.793\n",
      "epoch 7 train accuracy 0.851 f1_score 0.773 precision 0.840 recall 0.715\n",
      "epoch 7 test accuracy 0.405 f1_score 0.535 precision 0.375 recall 0.931\n",
      "epoch 8 train accuracy 0.863 f1_score 0.791 precision 0.861 recall 0.730\n",
      "epoch 8 test accuracy 0.425 f1_score 0.538 precision 0.382 recall 0.912\n",
      "epoch 9 train accuracy 0.876 f1_score 0.811 precision 0.878 recall 0.753\n",
      "epoch 9 test accuracy 0.368 f1_score 0.538 precision 0.368 recall 1.000\n",
      "epoch 10 train accuracy 0.878 f1_score 0.816 precision 0.880 recall 0.761\n",
      "epoch 10 test accuracy 0.640 f1_score 0.042 precision 0.889 recall 0.021\n",
      "epoch 11 train accuracy 0.875 f1_score 0.807 precision 0.889 recall 0.738\n",
      "epoch 11 test accuracy 0.634 f1_score 0.011 precision 0.667 recall 0.005\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "learning_rates = [4e-5]\n",
    "filters = [[4,5,6]]\n",
    "embedding_dim = [1000]\n",
    "num_of_kernel = [100]\n",
    "mode = ['char'] # jamo : 자음,모음 단위로 토큰화. char : 한글자 단위로 토큰화\n",
    "\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset('./data/train_hate_dataset_v2.txt')\n",
    "dataset_test = nlp.data.TSVDataset('./data/test_hate_dataset_v2.txt')\n",
    "\n",
    "best_f1 = 0\n",
    "for mode0 in mode:\n",
    "    for learning_rate in learning_rates:\n",
    "        for embedding_dim0 in embedding_dim:\n",
    "            for filters0 in filters:\n",
    "                for num_of_kernel0 in num_of_kernel:\n",
    "                    \n",
    "                    print('================')\n",
    "                    print('mode : {}'.format(mode0))\n",
    "                    print('lr : {}'.format(learning_rate))\n",
    "                    print('embedding_dim : {}'.format(embedding_dim0))\n",
    "                    print('filters : {}'.format(filters0))\n",
    "                    print('num_of_kernel : {}'.format(num_of_kernel0))\n",
    "                    print('================')\n",
    "                    \n",
    "                    data_train = CharDataset(dataset_train, 0, 1, mode=mode0)\n",
    "                    data_test = CharDataset(dataset_test, 0, 1, mode=mode0)\n",
    "\n",
    "                    train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, drop_last=True)\n",
    "                    test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, drop_last=True)\n",
    "\n",
    "                    model = Net(data_train.vocab_size, data_train.get_q3(), filters0, embedding_dim0, num_of_kernel0)\n",
    "                    model.to(device)\n",
    "                    criterion = nn.BCELoss()\n",
    "                    #criterion = nn.CrossEntropyLoss()\n",
    "                    #optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "                    #optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "                    #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "                    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.1)\n",
    "\n",
    "#                     scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
    "#                                                     lr_lambda=lambda epoch: 0.95 ** epoch,\n",
    "#                                                     last_epoch=-1,\n",
    "#                                                     verbose=False)\n",
    "\n",
    "                    for epoch in range(epochs):\n",
    "\n",
    "                        running_loss = 0.0\n",
    "                        correct = 0\n",
    "                        y_true, y_pred = [], []\n",
    "                        model.train()\n",
    "                        for i, data in enumerate(train_dataloader, 0):\n",
    "                            inputs, labels = data\n",
    "                            inputs = inputs.to(device)\n",
    "                            labels = labels.to(device)\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            #print('losses :', loss)\n",
    "                            running_loss = loss.item()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            #scheduler.step()\n",
    "                            pred = (outputs>0.5).to(torch.float)\n",
    "                            y_pred.extend(pred)\n",
    "                            y_true.extend(labels)\n",
    "                        y_true_cpu = [int(x) for x in y_true]\n",
    "                        y_pred_cpu = [int(x) for x in y_pred]\n",
    "                        correct = sum([(x==y) for x,y in zip(y_pred, y_true)])\n",
    "                        precision = precision_score(y_true_cpu, y_pred_cpu)\n",
    "                        recall = recall_score(y_true_cpu, y_pred_cpu)\n",
    "                        f1= f1_score(y_true_cpu, y_pred_cpu)\n",
    "                        print(\"epoch {} train accuracy {:.3f} f1_score {:.3f} precision {:.3f} recall {:.3f}\".format(epoch+1, correct / (len(y_pred)), f1, precision, recall))\n",
    "\n",
    "                        model.eval()\n",
    "                        y_true, y_pred = [], []\n",
    "                        running_loss = 0.0\n",
    "                        correct = 0\n",
    "                        for i, data in enumerate(test_dataloader, 0):\n",
    "                            inputs, labels = data\n",
    "                            inputs = inputs.to(device)\n",
    "                            labels = labels.to(device)\n",
    "                            outputs = model(inputs)\n",
    "                            pred = (outputs>0.5).to(torch.float)\n",
    "                            loss = criterion(outputs, labels)\n",
    "                            running_loss += loss.item()\n",
    "                            y_pred.extend(pred)\n",
    "                            y_true.extend(labels)\n",
    "                        y_true_cpu = [x.cpu() for x in y_true]\n",
    "                        y_pred_cpu = [x.cpu() for x in y_pred]\n",
    "                        correct = sum([(x==y) for x,y in zip(y_pred, y_true)])\n",
    "                        precision = precision_score(y_true_cpu, y_pred_cpu)\n",
    "                        recall = recall_score(y_true_cpu, y_pred_cpu)\n",
    "                        f1 = f1_score(y_true_cpu, y_pred_cpu)\n",
    "                        print(\"epoch {} test accuracy {:.3f} f1_score {:.3f} precision {:.3f} recall {:.3f}\".format(epoch+1, correct / (len(y_pred)), f1, precision, recall))\n",
    "                        if epoch == 10 and f1 < 0.3:\n",
    "                            break\n",
    "                            \n",
    "                        if f1 > best_f1:\n",
    "                            best_model = model\n",
    "                            best_f1 = f1\n",
    "                    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6475333",
   "metadata": {},
   "source": [
    "## save 1dcnn_sentencepiece.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b17d4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), './check_point/1dcnn_sentencepiece.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "92242013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5424"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d1483eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(6403, 1000)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_0): Conv1d(1000, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_1): Conv1d(1000, 100, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(1000, 100, kernel_size=(6,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(data_train.vocab_size, data_train.get_q3(), [4,5,6], 1000, 100)\n",
    "model.load_state_dict(torch.load('./check_point/1dcnn_sentencepiece.pt'))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e5f1d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "prefix = './'\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "learning_rates = [4e-5]\n",
    "filters = [[4,5,6]]\n",
    "embedding_dim = [1000]\n",
    "num_of_kernel = [100]\n",
    "mode = ['sentencepiece'] # jamo : 자음,모음 단위로 토큰화. char : 한글자 단위로 토큰화\n",
    "\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset('./data/train_hate_dataset_v2.txt')\n",
    "dataset_test = nlp.data.TSVDataset('./data/test_hate_dataset_v2.txt')\n",
    "                                   \n",
    "data_train = CharDataset(dataset_train, 0, 1, mode=mode[0])\n",
    "data_test = CharDataset(dataset_test, 0, 1, mode=mode[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ec630da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 443,  360,   41, 4763, 4205, 5250, 6402, 6402, 6402, 6402, 6402, 6402,\n",
      "        6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402,\n",
      "        6402, 6402, 6402, 6402, 6402, 6402])\n",
      "['▁', 'ᄊ', '1', '발', '놈', '아', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "tensor([[ 443,  360,   41, 4763, 4205, 5250, 6402, 6402, 6402, 6402, 6402, 6402,\n",
      "         6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402, 6402,\n",
      "         6402, 6402, 6402, 6402, 6402, 6402]], device='cuda:0')\n",
      "tensor(0.8719, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = 'ㅆ1발놈아'\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "    chars = korean.sub('', sentence)\n",
    "    chars = data_train.make_token(chars)\n",
    "    if len(chars) < data_train.get_q3():\n",
    "        need_pad = data_train.get_q3() - len(chars)\n",
    "        chars.extend(['<PAD>']*need_pad)\n",
    "    else:\n",
    "        chars = chars[:data_train.q3]\n",
    "    chars = torch.tensor([data_train.char2idx[x] for x in chars]).to(torch.int64)\n",
    "    print(chars)\n",
    "    chars = chars.to(device)\n",
    "    chars = torch.unsqueeze(chars, 0)\n",
    "    prt = [data_train.idx2char[x.item()] for x in chars.squeeze()]\n",
    "    print(prt)\n",
    "    print(chars)\n",
    "    outputs = model(chars)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2bf86c",
   "metadata": {},
   "source": [
    "## save 1dcnn_jamo.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "77689219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(54, 1000)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_0): Conv1d(1000, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_1): Conv1d(1000, 100, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(1000, 100, kernel_size=(6,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "49a4cb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5510373443983403"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b2020c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), './check_point/1dcnn_jamo.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a6568534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "prefix = './'\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "learning_rates = [4e-5]\n",
    "filters = [[4,5,6]]\n",
    "embedding_dim = [1000]\n",
    "num_of_kernel = [100]\n",
    "mode = ['jamo'] # jamo : 자음,모음 단위로 토큰화. char : 한글자 단위로 토큰화\n",
    "\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset('./data/train_hate_dataset_v2.txt')\n",
    "dataset_test = nlp.data.TSVDataset('./data/test_hate_dataset_v2.txt')\n",
    "                                   \n",
    "data_train = CharDataset(dataset_train, 0, 1, mode='jamo')\n",
    "data_test = CharDataset(dataset_test, 0, 1, mode='jamo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6bb33a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(54, 1000)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_0): Conv1d(1000, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_1): Conv1d(1000, 100, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(1000, 100, kernel_size=(6,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(data_train.vocab_size, data_train.get_q3(), [4,5,6], 1000, 100)\n",
    "model.load_state_dict(torch.load('./check_point/1dcnn_jamo.pt'))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "96c247b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 37, 23, 21, 51,  5, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "        53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "        53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "        53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "        53, 53, 53, 53, 53, 53, 53, 53, 53])\n",
      "['ㅂ', 'ㅕ', 'ㅇ', 'ㅅ', 'ㅣ', 'ㄴ', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "tensor([[18, 37, 23, 21, 51,  5, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "         53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "         53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "         53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53,\n",
      "         53, 53, 53, 53, 53, 53, 53, 53, 53]], device='cuda:0')\n",
      "tensor(0.5163, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sentence = ''\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "    chars = korean.sub('', sentence)\n",
    "    chars = data_train.make_token(chars)\n",
    "    if len(chars) < data_train.get_q3():\n",
    "        need_pad = data_train.get_q3() - len(chars)\n",
    "        chars.extend(['<PAD>']*need_pad)\n",
    "    else:\n",
    "        chars = chars[:data_train.q3]\n",
    "    chars = torch.tensor([data_train.char2idx[x] for x in chars]).to(torch.int64)\n",
    "    print(chars)\n",
    "    chars = chars.to(device)\n",
    "    chars = torch.unsqueeze(chars, 0)\n",
    "    prt = [data_train.idx2char[x.item()] for x in chars.squeeze()]\n",
    "    print(prt)\n",
    "    print(chars)\n",
    "    outputs = model(chars)\n",
    "    print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d2113b",
   "metadata": {},
   "source": [
    "## save 1dcnn_char.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f72bbd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(1668, 1000)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_0): Conv1d(1000, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_1): Conv1d(1000, 100, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(1000, 100, kernel_size=(6,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "53f6d400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5409356725146198"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b79bc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(best_model.state_dict(), './check_point/1dcnn_char.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "704002f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "prefix = './'\n",
    "epochs = 30\n",
    "batch_size = 128\n",
    "learning_rates = [4e-5]\n",
    "filters = [[4,5,6]]\n",
    "embedding_dim = [1000]\n",
    "num_of_kernel = [100]\n",
    "mode = ['char'] # jamo : 자음,모음 단위로 토큰화. char : 한글자 단위로 토큰화\n",
    "\n",
    "torch.manual_seed(21)\n",
    "np.random.seed(21)\n",
    "random.seed(21)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "dataset_train = nlp.data.TSVDataset('./data/train_hate_dataset_v2.txt')\n",
    "dataset_test = nlp.data.TSVDataset('./data/test_hate_dataset_v2.txt')\n",
    "                                   \n",
    "data_train = CharDataset(dataset_train, 0, 1, mode='char')\n",
    "data_test = CharDataset(dataset_test, 0, 1, mode='char')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "97d6f864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(1668, 1000)\n",
       "  (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_0): Conv1d(1000, 100, kernel_size=(4,), stride=(1,))\n",
       "  (conv_1): Conv1d(1000, 100, kernel_size=(5,), stride=(1,))\n",
       "  (conv_2): Conv1d(1000, 100, kernel_size=(6,), stride=(1,))\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (bn2): BatchNorm1d(300, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=300, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(data_train.vocab_size, data_train.get_q3(), [4,5,6], 1000, 100)\n",
    "model.load_state_dict(torch.load('./check_point/1dcnn_char.pt'))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1446592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 387, 1223,   46, 1106,  291,   17,   17, 1667, 1667, 1667, 1667, 1667,\n",
      "        1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667,\n",
      "        1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667])\n",
      "['돼', '지', '같', '은', '놈', 'ㅋ', 'ㅋ', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
      "tensor([[ 387, 1223,   46, 1106,  291,   17,   17, 1667, 1667, 1667, 1667, 1667,\n",
      "         1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667,\n",
      "         1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667, 1667]],\n",
      "       device='cuda:0')\n",
      "tensor(0.5369, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model = best_model\n",
    "sentence = '돼지같은놈 ㅋㅋ'\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    korean = re.compile('[^1!ㄱ-ㅣ가-힣]+')\n",
    "    chars = korean.sub('', sentence)\n",
    "    chars = data_train.make_token(chars)\n",
    "    if len(chars) < data_train.get_q3():\n",
    "        need_pad = data_train.get_q3() - len(chars)\n",
    "        chars.extend(['<PAD>']*need_pad)\n",
    "    else:\n",
    "        chars = chars[:data_train.q3]\n",
    "    chars = torch.tensor([data_train.char2idx[x] for x in chars]).to(torch.int64)\n",
    "    print(chars)\n",
    "    chars = chars.to(device)\n",
    "    chars = torch.unsqueeze(chars, 0)\n",
    "    prt = [data_train.idx2char[x.item()] for x in chars.squeeze()]\n",
    "    print(prt)\n",
    "    print(chars)\n",
    "    outputs = model(chars)\n",
    "    print(outputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
