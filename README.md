자연어처리 분야는 기존 통계기반 모델에서 발전해왔으며, 요즈음에는 딥러닝을 통하여 좋은 성능을 달성하고 있다. 하지만, 딥러닝 모델의 높은 성능을 위해서는 대용량의 학습 데이터셋이 필요로 하기 마련이다. 이때, 무분별한 데이터 수집 속 욕설 및 폭언과 같은 데이터들은 모델의 성능을 저하할 수 있으며, 더 나아가 윤리적인 문제가 발생할 수 있다. 이 프로젝트는 수집된 한국어 데이터에서 욕설 및 폭언이 담긴 데이터를 제외하는 모델을 구축하고자 한다.

본 실험에서는 한국어에 맞는 여러 토큰화 방법을 비교하며 적합한 토큰화 방법을 찾아내고 추가적으로, Computer Vision에 주로 쓰이는 CNN 모델의 자연어처리 접목 가능성을 실험하고자 한다. 이에 비교 모델로, SKT의 KoBERT 모델과 RNN계열중 하나인 GRU 모델을 함께 실험하며 결과를 비교해보고자 한다.

https://www.notion.so/75247b0f81cb4ed6bdb7bd419164250d
